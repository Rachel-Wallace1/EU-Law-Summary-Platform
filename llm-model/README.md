# LLM Inference

- LLM related code for hosting final model for inferencing.

- Treat this as backend that hosts the LLM
- This is the production LLM. The most up to date, final model that the web application pulls data from.